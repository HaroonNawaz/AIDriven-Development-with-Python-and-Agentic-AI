---
# ğŸ“ What is LLM - Basic Flashcards
## ğŸ“š Level 1: Foundational Knowledge

*Visual Flashcard Format - Study Guide with Flip-Card Design*

---

## ğŸ¯ CARD 1ï¸âƒ£ | LLM Definition | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is a Large Language Model (LLM)?          â•‘
â•‘                                                     â•‘
â•‘                                                     â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  An artificial intelligence model trained on       â•‘
â•‘  massive amounts of text data to understand and    â•‘
â•‘  generate human language.                          â•‘
â•‘                                                     â•‘
â•‘  ğŸ”§ MECHANISM:                                      â•‘
â•‘  â€¢ Uses neural networks (transformer)              â•‘
â•‘  â€¢ Predicts next token given previous text         â•‘
â•‘  â€¢ 175B to 540B+ parameters                        â•‘
â•‘                                                     â•‘
â•‘  ğŸ“š REAL-WORLD EXAMPLES:                           â•‘
â•‘  ChatGPT, GPT-4, Claude, Llama 2                  â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  "LARGE LANGUAGE MODEL"                           â•‘
â•‘  L - Large (billions of parameters)                â•‘
â•‘  L - Language (text-based)                         â•‘
â•‘  M - Model (AI system)                             â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 3, 6, 14                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: DEFINITION   â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 2ï¸âƒ£ | Token | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is a TOKEN in the context of LLMs?       â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  The smallest unit of text that an LLM processes.  â•‘
â•‘                                                     â•‘
â•‘  ğŸ“Š TYPES OF TOKENS:                               â•‘
â•‘  ğŸ”¹ Word: "hello"                                   â•‘
â•‘  ğŸ”¹ Subword: "hel", "lo"                            â•‘
â•‘  ğŸ”¹ Character: individual letters                  â•‘
â•‘                                                     â•‘
â•‘  ğŸ“ PRACTICAL EXAMPLE:                              â•‘
â•‘  Sentence: "Hello world"                           â•‘
â•‘                                                     â•‘
â•‘  Option 1: ["Hello", "world"] = 2 tokens          â•‘
â•‘  Option 2: ["Hel", "lo", "wor", "ld"] = 4 tokens  â•‘
â•‘                                                     â•‘
â•‘  ğŸ’­ KEY INSIGHT:                                    â•‘
â•‘  LLMs don't work with words directly â€”             â•‘
â•‘  they work with tokens!                            â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  TOKEN = Smallest building block of text           â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 1, 6, 16                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: TERMINOLOGY  â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 3ï¸âƒ£ | Transformer Architecture | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is TRANSFORMER ARCHITECTURE?              â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  A neural network design introduced in 2017.       â•‘
â•‘  Foundation for ALL modern LLMs.                   â•‘
â•‘                                                     â•‘
â•‘  ğŸ”‘ KEY COMPONENTS:                                 â•‘
â•‘  âš™ï¸  Attention Mechanisms (look at relevant info)  â•‘
â•‘  âš™ï¸  Parallel Processing (process all at once)     â•‘
â•‘  âš™ï¸  Multiple Layers (20-100+ stacked)              â•‘
â•‘                                                     â•‘
â•‘  ğŸš€ WHY REVOLUTIONARY:                              â•‘
â•‘  âœ“ Process text FASTER (parallel, not sequential) â•‘
â•‘  âœ“ Understand LONGER contexts                      â•‘
â•‘  âœ“ Enabled scaling to BILLIONS of parameters      â•‘
â•‘                                                     â•‘
â•‘  ğŸ“Š BREAKTHROUGH PAPER:                             â•‘
â•‘  "Attention Is All You Need" (2017)                â•‘
â•‘  Changed the entire AI landscape                   â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Transformer = Attention + Layers + Speed         â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 4, 5, 14, 20                    â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: ARCHITECTURE â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 4ï¸âƒ£ | Attention Mechanism | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is the ATTENTION MECHANISM?               â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  Mathematical technique allowing neural networks   â•‘
â•‘  to focus on relevant parts of input text.         â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ WHAT IT DOES:                                   â•‘
â•‘  â†’ Dynamically weights importance of different    â•‘
â•‘    parts of input                                  â•‘
â•‘  â†’ Lets model understand which words matter        â•‘
â•‘  â†’ Enables long-range understanding                â•‘
â•‘                                                     â•‘
â•‘  ğŸ“ REAL EXAMPLE:                                   â•‘
â•‘  Sentence: "The ball rolled down the hill         â•‘
â•‘            and hit a rock. It bounced high."      â•‘
â•‘                                                     â•‘
â•‘  Without attention: Can't remember "ball"          â•‘
â•‘  With attention: KNOWS "It" = "ball" âœ“            â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  WHY IT MATTERS:                                 â•‘
â•‘  Solves the "looking back" problem                 â•‘
â•‘  Understands relationships between distant words  â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Attention = Focus on what's important             â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 3, 13, 14                       â•‘
â•‘  â­ DIFFICULTY: â˜…â˜…â˜† MEDIUM | Category: MECHANISM  â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 5ï¸âƒ£ | Parameters | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What are PARAMETERS in neural networks?       â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  Learned weights and biases within a neural       â•‘
â•‘  network. Adjusted during training to improve     â•‘
â•‘  predictions.                                      â•‘
â•‘                                                     â•‘
â•‘  ğŸ“Š SIZE EXAMPLES:                                  â•‘
â•‘  Small model: 7 billion (7B) parameters            â•‘
â•‘  Medium model: 70 billion (70B) parameters         â•‘
â•‘  Large model: 175 billion (175B) parameters        â•‘
â•‘  Very Large: 540+ billion (540B+) parameters       â•‘
â•‘                                                     â•‘
â•‘  ğŸ”§ RELATIONSHIP TO CAPABILITY:                    â•‘
â•‘  More parameters = Better capability               â•‘
â•‘  (but with diminishing returns)                    â•‘
â•‘                                                     â•‘
â•‘  ğŸ’° RESOURCE IMPACT:                                â•‘
â•‘  More parameters = Harder to train                 â•‘
â•‘  More parameters = Slower inference                â•‘
â•‘  More parameters = More memory needed              â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Parameters = The "knobs" the model learns         â•‘
â•‘             to turn for better predictions         â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 6, 7, 12                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: SCALE        â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 6ï¸âƒ£ | Training Objective | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is the PRIMARY TRAINING OBJECTIVE        â•‘
â•‘     of language models?                            â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ PRIMARY OBJECTIVE:                              â•‘
â•‘  Predict the most likely NEXT TOKEN given all      â•‘
â•‘  previous tokens in a sequence.                    â•‘
â•‘                                                     â•‘
â•‘  ğŸ“‹ THE TRAINING PROCESS:                           â•‘
â•‘  1. Show model: "The cat sat on the..."            â•‘
â•‘  2. Model predicts next: "mat" or "couch"         â•‘
â•‘  3. Compare to actual: "mat" âœ“                     â•‘
â•‘  4. Update weights to improve                      â•‘
â•‘  5. Repeat billions of times                       â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ AMAZING FACT:                                   â•‘
â•‘  This SIMPLE objective, when scaled to             â•‘
â•‘  BILLIONS of parameters and trillions of           â•‘
â•‘  training examples, produces SOPHISTICATED         â•‘
â•‘  language understanding!                           â•‘
â•‘                                                     â•‘
â•‘  ğŸ’­ KEY INSIGHT:                                    â•‘
â•‘  Next-token prediction = The secret sauce          â•‘
â•‘  that makes LLMs work!                             â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Training = Learn to predict what comes next       â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 1, 2, 16                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: TRAINING     â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 7ï¸âƒ£ | Model Size Range | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is the typical SIZE RANGE of modern      â•‘
â•‘     Large Language Models?                         â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ“Š TYPICAL RANGE: 7 billion to 540+ billion      â•‘
â•‘                   parameters                       â•‘
â•‘                                                     â•‘
â•‘  ğŸ“‹ REAL-WORLD EXAMPLES:                            â•‘
â•‘  ğŸ”¹ Small:    LLaMA-7B = 7 billion                 â•‘
â•‘  ğŸ”¹ Small+:   LLaMA-13B = 13 billion               â•‘
â•‘  ğŸ”¹ Medium:   LLaMA-70B = 70 billion               â•‘
â•‘  ğŸ”¹ Large:    GPT-3.5 = ~175 billion              â•‘
â•‘  ğŸ”¹ Large+:   GPT-4 = 100B+ billion               â•‘
â•‘  ğŸ”¹ Massive:  PaLM-2 = 340+ billion                â•‘
â•‘                                                     â•‘
â•‘  ğŸ” SCALE PERSPECTIVE:                              â•‘
â•‘  1 billion = 1,000,000,000                         â•‘
â•‘  175 billion = 175 billion "knobs" to turn         â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¡ KEY TAKEAWAY:                                   â•‘
â•‘  Bigger models usually = Better performance        â•‘
â•‘  BUT bigger = More expensive, slower               â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Modern LLMs range BILLIONS of parameters          â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 5, 8, 12, 13                    â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: SCALE        â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 8ï¸âƒ£ | Training Data Scale | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ How much TRAINING DATA                         â•‘
â•‘     (measured in tokens) are modern LLMs          â•‘
â•‘     typically trained on?                          â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ“Š TYPICAL RANGE: 300 billion to 2+ trillion     â•‘
â•‘                   tokens                           â•‘
â•‘                                                     â•‘
â•‘  ğŸ”¢ TO PUT IN PERSPECTIVE:                          â•‘
â•‘  1 trillion tokens â‰ˆ 800+ billion words            â•‘
â•‘  Average book â‰ˆ 100,000 words                      â•‘
â•‘  So 1T tokens â‰ˆ 8 MILLION BOOKS!                   â•‘
â•‘                                                     â•‘
â•‘  ğŸ“š TRAINING DATA SOURCES:                          â•‘
â•‘  â€¢ Books and literature                            â•‘
â•‘  â€¢ Websites and articles                           â•‘
â•‘  â€¢ Scientific papers                               â•‘
â•‘  â€¢ Code repositories                               â•‘
â•‘  â€¢ Forum discussions                               â•‘
â•‘  â€¢ News articles                                   â•‘
â•‘  â€¢ And much more...                                â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¡ KEY INSIGHT:                                    â•‘
â•‘  More data + More parameters = Better model        â•‘
â•‘                                                     â•‘
â•‘  âš ï¸  IMPORTANT NOTE:                                 â•‘
â•‘  Training is ONE-TIME (takes weeks/months)         â•‘
â•‘  Using (inference) is ONGOING (milliseconds)       â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Massive text corpus = Billions to trillions       â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 6, 7, 11                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: DATA         â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 9ï¸âƒ£ | Inference | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What does "INFERENCE" mean in LLMs?           â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  Using a trained model to generate text or make    â•‘
â•‘  predictions. Inference happens AFTER training is  â•‘
â•‘  completeâ€”it's when the model responds to user     â•‘
â•‘  prompts.                                          â•‘
â•‘                                                     â•‘
â•‘  â±ï¸ TIMING:                                         â•‘
â•‘  Training = Weeks/months (one-time)               â•‘
â•‘  Inference = Milliseconds (ongoing)               â•‘
â•‘                                                     â•‘
â•‘  ğŸ“š ANALOGY:                                        â•‘
â•‘  Training = Going to school and learning          â•‘
â•‘  Inference = Using what you learned to answer     â•‘
â•‘                                                    â•‘
â•‘  ğŸ”„ THE FLOW:                                       â•‘
â•‘  1. Model is trained (billions of examples)       â•‘
â•‘  2. Training stops, weights are fixed             â•‘
â•‘  3. You give it a prompt                          â•‘
â•‘  4. It INFERS (predicts) your response            â•‘
â•‘  5. Response is generated token by token          â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Inference = Using trained model to generate      â•‘
â•‘             output to user prompts                â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 6, 10, 16                      â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: OPERATIONS  â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD ğŸ”Ÿ | Hallucination Definition | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is HALLUCINATION in LLMs?                â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  Generation of PLAUSIBLE-SOUNDING but FALSE        â•‘
â•‘  information. The LLM sounds confident and correct â•‘
â•‘  but is actually wrong.                            â•‘
â•‘                                                     â•‘
â•‘  ğŸ” WHY IT HAPPENS:                                 â•‘
â•‘  LLMs predict tokens based on learned patterns    â•‘
â•‘  from training data, NOT from a knowledge database â•‘
â•‘  or fact-checking system. They guess what         â•‘
â•‘  should come next based on probability.           â•‘
â•‘                                                     â•‘
â•‘  âŒ REAL-WORLD EXAMPLES:                            â•‘
â•‘  â€¢ Inventing fake citations/research papers       â•‘
â•‘  â€¢ Making up historical facts that sound real     â•‘
â•‘  â€¢ Creating plausible but false statistics        â•‘
â•‘  â€¢ Confidently claiming events that never happenedâ•‘
â•‘                                                     â•‘
â•‘  âš ï¸ KEY DANGER:                                     â•‘
â•‘  Model doesn't SAY it's uncertain. It delivers   â•‘
â•‘  false info with high confidence!                 â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ CRITICAL INSIGHT:                              â•‘
â•‘  Hallucination â‰  Bug. It's inherent to how       â•‘
â•‘  LLMs work (predicting next token).              â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Hallucination = Confident false information      â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 6, 19, (Advanced-5)            â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: LIMITATIONS â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£1ï¸âƒ£ | Scaling Laws | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What are SCALING LAWS in LLMs?                â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  PREDICTABLE PATTERNS showing that model            â•‘
â•‘  performance improves as you increase model size  â•‘
â•‘  and training data. "Bigger usually = Better"     â•‘
â•‘                                                     â•‘
â•‘  ğŸ“ˆ THE SCALING PATTERN:                            â•‘
â•‘  Doubling model size    â†’ Performance improves    â•‘
â•‘  Doubling training data â†’ Performance improves    â•‘
â•‘  This pattern is CONSISTENT and PREDICTABLE!      â•‘
â•‘                                                     â•‘
â•‘  ğŸ”¬ EMPIRICAL EVIDENCE:                             â•‘
â•‘  Researchers discovered these laws by testing     â•‘
â•‘  - GPT-2 (1.5B) < GPT-3 (175B) < GPT-4 (540B+)   â•‘
â•‘  Each larger model showed predictable better      â•‘
â•‘  performance, matching scaling law predictions    â•‘
â•‘                                                     â•‘
â•‘  ğŸ’ª WHY IMPORTANT:                                  â•‘
â•‘  Allows researchers to PREDICT performance        â•‘
â•‘  improvements without building expensive models   â•‘
â•‘  Drives billion-dollar investment decisions       â•‘
â•‘                                                     â•‘
â•‘  âš ï¸ CAVEAT:                                         â•‘
â•‘  At MASSIVE scales, returns diminish              â•‘
â•‘  You can't scale foreverâ€”trade-offs matter        â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Scaling Laws = Bigger data + bigger model        â•‘
â•‘               = Predictably better capability     â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 5, 7, 8, 13                    â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: SCALE       â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£2ï¸âƒ£ | Emergence Phenomenon | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What does EMERGENCE mean in                    â•‘
â•‘     large language models?                         â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  UNEXPECTED NEW ABILITIES that appear in larger    â•‘
â•‘  models but don't exist in smaller models.         â•‘
â•‘  Suggests QUALITATIVE DIFFERENCES, not just       â•‘
â•‘  gradual improvements.                             â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ THE SURPRISING PHENOMENON:                     â•‘
â•‘  Small model (1B params): Cannot do X             â•‘
â•‘  Medium model (7B params): Still cannot do X      â•‘
â•‘  Large model (175B params): SUDDENLY can do X!    â•‘
â•‘  â†’ Ability appeared "out of nowhere"              â•‘
â•‘                                                     â•‘
â•‘  ğŸ” REAL-WORLD EXAMPLE:                             â•‘
â•‘  IN-CONTEXT LEARNING:                             â•‘
â•‘  Models <100B: Cannot learn from examples in      â•‘
â•‘               the prompt itself                   â•‘
â•‘  Models >100B: CAN learn from examples in prompt  â•‘
â•‘  This ability EMERGES at ~100B parameters!        â•‘
â•‘                                                     â•‘
â•‘  ğŸ¤” WHY IT MATTERS:                                 â•‘
â•‘  Suggests qualitative change, not just quantity   â•‘
â•‘  Larger models might have capabilities we        â•‘
â•‘  haven't discovered yet!                         â•‘
â•‘  Implies consciousness/understanding? (Unknown!)  â•‘
â•‘                                                     â•‘
â•‘  ğŸ’­ OPEN QUESTION:                                  â•‘
â•‘  Is emergence a sign of true understanding?       â•‘
â•‘  Or just complex pattern-matching at scale?       â•‘
â•‘  â†’ Still being debated by researchers!            â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Emergence = Surprise new abilities in big models â•‘
â•‘            = Models change QUALITATIVELY at scale â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 5, 7, 11, (Intermediate-8)     â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: PHENOMENA   â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£3ï¸âƒ£ | Neural Networks | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is a NEURAL NETWORK?                      â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  A COMPUTATIONAL SYSTEM inspired by how biologicalâ•‘
â•‘  brains work. Made of layers of artificial neuronsâ•‘
â•‘  connected through adjustable weights.             â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  BIOLOGICAL INSPIRATION:                         â•‘
â•‘  Real brain neurons:                              â•‘
â•‘    Fire signals (input) â†’ Synapse connection      â•‘
â•‘    â†’ Other neuron fires (output)                  â•‘
â•‘                                                     â•‘
â•‘  Artificial neurons:                              â•‘
â•‘    Numerical input â†’ Mathematical operation       â•‘
â•‘    â†’ Numerical output                             â•‘
â•‘                                                     â•‘
â•‘  ğŸ—ï¸ STRUCTURE:                                      â•‘
â•‘  Input Layer (you provide data)                   â•‘
â•‘      â†“                                             â•‘
â•‘  Hidden Layers (many! processing happens)         â•‘
â•‘      â†“                                             â•‘
â•‘  Output Layer (model's prediction)                â•‘
â•‘                                                     â•‘
â•‘  ğŸ”„ HOW IT LEARNS:                                  â•‘
â•‘  1. Input data flows through network             â•‘
â•‘  2. Makes a prediction                           â•‘
â•‘  3. Compare to correct answer                    â•‘
â•‘  4. Calculate error                              â•‘
â•‘  5. Adjust all weights to reduce error           â•‘
â•‘  6. Repeat billions of times                     â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¡ KEY INSIGHT:                                    â•‘
â•‘  Neural networks are the FOUNDATION of modern     â•‘
â•‘  AI. LLMs = Transformers = Special neural network â•‘
â•‘  architecture.                                    â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Neural Network = Interconnected layers of        â•‘
â•‘                  adjustable mathematical units    â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 3, 4, 14                       â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: ARCHITECTUREâ•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£4ï¸âƒ£ | Context Length | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is CONTEXT LENGTH in LLMs?               â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  The MAXIMUM NUMBER OF TOKENS the model can       â•‘
â•‘  consider AT ONCE when generating responses.      â•‘
â•‘  Longer context = Model can "see" more text      â•‘
â•‘                                                     â•‘
â•‘  ğŸ“ MODERN CONTEXT LENGTHS:                         â•‘
â•‘  Typical range: 2,000 to 100,000+ tokens          â•‘
â•‘  GPT-4 Turbo: 128,000 tokens                      â•‘
â•‘  Claude 3: Up to 200,000 tokens                   â•‘
â•‘  Some models: 1,000,000+ tokens (emerging)        â•‘
â•‘                                                     â•‘
â•‘  ğŸ”¢ TO PUT IN PERSPECTIVE:                          â•‘
â•‘  2,000 tokens â‰ˆ 1,500 words                       â•‘
â•‘  128,000 tokens â‰ˆ 100,000 words                   â•‘
â•‘  1 typical document â‰ˆ 5,000 words                 â•‘
â•‘  So GPT-4 can see ~20 full documents at once!     â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¼ PRACTICAL IMPACT:                               â•‘
â•‘  Short context (2K): Can't remember long convos   â•‘
â•‘  Long context (128K): Can analyze full books      â•‘
â•‘  Ultra-long (1M): Can digest entire code bases    â•‘
â•‘                                                     â•‘
â•‘  âš¡ TRADEOFF:                                       â•‘
â•‘  Longer context = More powerful BUT               â•‘
â•‘  Longer context = Slower inference                â•‘
â•‘  Longer context = Higher cost                     â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ LIMITATION:                                     â•‘
â•‘  Even with long context, info far from end       â•‘
â•‘  is "attended to" less (loses importance)         â•‘
â•‘  = "Lost in the middle" problem                   â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Context Length = How much text the model        â•‘
â•‘                  can "see" and remember at once  â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 2, 3, 4                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: CAPABILITIESâ•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£5ï¸âƒ£ | Text Tokenization | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is TOKENIZATION?                         â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  The PROCESS OF BREAKING TEXT into smaller units  â•‘
â•‘  called tokens. Different schemes create          â•‘
â•‘  different token sequences from the same text.    â•‘
â•‘                                                     â•‘
â•‘  ğŸ”„ THE PROCESS:                                    â•‘
â•‘  Original text: "Hello world"                     â•‘
â•‘           â†“ (tokenization)                        â•‘
â•‘  Option 1: ["Hello", "world"]         (2 tokens)  â•‘
â•‘  Option 2: ["Hel", "lo", "wor", "ld"] (4 tokens)  â•‘
â•‘  Option 3: ["H", "e", "l", "l", "o", ...] (chars) â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ TOKENIZATION STRATEGIES:                        â•‘
â•‘  â€¢ Word tokenization: Split on spaces/punctuation â•‘
â•‘  â€¢ Byte-pair encoding (BPE): Statistical merging  â•‘
â•‘  â€¢ Subword tokenization: Balance of two above     â•‘
â•‘  â€¢ Character-level: Every single character        â•‘
â•‘                                                     â•‘
â•‘  ğŸ” REAL IMPACT:                                    â•‘
â•‘  Different tokenization:                          â•‘
â•‘  â€¢ Changes token count (affects cost!)            â•‘
â•‘  â€¢ Affects how model "sees" text                  â•‘
â•‘  â€¢ Different languages = different token counts   â•‘
â•‘  â€¢ Chinese typically has more tokens than English â•‘
â•‘                                                     â•‘
â•‘  ğŸ’° COST IMPLICATIONS:                              â•‘
â•‘  You pay by token, not by word!                   â•‘
â•‘  Inefficient tokenization = Higher costs          â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¡ INTERESTING NOTE:                               â•‘
â•‘  "hello" = 1 token                                â•‘
â•‘  "Hello" = 1 token                                â•‘
â•‘  "HELLO" = 1 token                                â•‘
â•‘  But punctuation changes count significantly!     â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Tokenization = Breaking text into discrete       â•‘
â•‘               token units the model processes     â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 2, 1, 6                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: PROCESSING  â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£6ï¸âƒ£ | Knowledge Cutoff | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is a KNOWLEDGE CUTOFF in LLMs?           â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  The TRAINING DATA COMPLETION DATE. LLMs have     â•‘
â•‘  no knowledge of events or information after      â•‘
â•‘  this date because they weren't included in       â•‘
â•‘  the training data.                               â•‘
â•‘                                                     â•‘
â•‘  ğŸ“… REAL EXAMPLES:                                  â•‘
â•‘  GPT-3.5: Knowledge cutoff April 2023             â•‘
â•‘  GPT-4: Knowledge cutoff April 2024               â•‘
â•‘  Claude 3: Knowledge cutoff early 2024            â•‘
â•‘  LLaMA: Knowledge cutoff varies by version        â•‘
â•‘                                                     â•‘
â•‘  âŒ PRACTICAL LIMITATION:                           â•‘
â•‘  Model doesn't know: 2024 events (if cutoff 2023) â•‘
â•‘  Model doesn't know: Recent discoveries           â•‘
â•‘  Model doesn't know: New laws/regulations         â•‘
â•‘  Model doesn't know: Current happenings           â•‘
â•‘  â†’ But it will hallucinate answers anyway!        â•‘
â•‘                                                     â•‘
â•‘  âš ï¸ CRITICAL DANGER:                                â•‘
â•‘  Model doesn't SAY "I don't know."                â•‘
â•‘  Instead it makes stuff up that sounds plausible! â•‘
â•‘  â†’ This is HALLUCINATION                          â•‘
â•‘                                                     â•‘
â•‘  ğŸ”§ WORKAROUNDS:                                    â•‘
â•‘  1. Retrieval Augmentation: Give model current    â•‘
â•‘     information in the prompt                    â•‘
â•‘  2. Web Search Integration: Model can lookup      â•‘
â•‘     current facts                                â•‘
â•‘  3. Fine-tuning: Update model with new data      â•‘
â•‘     (expensive, creates new model)               â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¡ KEY INSIGHT:                                    â•‘
â•‘  Knowledge cutoff is FEATURE of training data     â•‘
â•‘  not a BUGâ€”it's inherent to how models work      â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Knowledge Cutoff = Latest date the model        â•‘
â•‘                     has knowledge from training   â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 8, 6, 11                       â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: LIMITATIONS â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£7ï¸âƒ£ | Fine-tuning | â­â­ IMPORTANT

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ What is FINE-TUNING an LLM?                    â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ DEFINITION:                                     â•‘
â•‘  SPECIALIZED TRAINING of a pre-trained model     â•‘
â•‘  on domain-specific data to adapt it to           â•‘
â•‘  particular tasks or specialized domains.         â•‘
â•‘                                                     â•‘
â•‘  ğŸ”„ THE FINE-TUNING PROCESS:                       â•‘
â•‘  1. Start with: Pre-trained general model (GPT-4) â•‘
â•‘  2. Get: Domain-specific training data (medical)  â•‘
â•‘  3. Train: On this smaller dataset (fine-tuning) â•‘
â•‘  4. Result: Model specialized for domain          â•‘
â•‘                                                     â•‘
â•‘  ğŸ“š REAL-WORLD EXAMPLES:                            â•‘
â•‘  Base model: GPT-3.5 (general)                    â•‘
â•‘       â†“ fine-tune on medical texts                â•‘
â•‘  Result: Medical diagnosis assistant              â•‘
â•‘                                                     â•‘
â•‘  Base model: GPT-3.5 (general)                    â•‘
â•‘       â†“ fine-tune on legal documents              â•‘
â•‘  Result: Legal document analyzer                  â•‘
â•‘                                                     â•‘
â•‘  âš¡ EFFICIENCY ADVANTAGES:                          â•‘
â•‘  Original training: Months, billions $ in compute â•‘
â•‘  Fine-tuning: Days-weeks, hundreds-thousands $    â•‘
â•‘  Much faster and cheaper!                        â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¾ DATA REQUIREMENTS:                              â•‘
â•‘  Original training: Billions-trillions of tokens  â•‘
â•‘  Fine-tuning: Thousands to millions of examples  â•‘
â•‘  Much less data needed!                          â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ USE CASES:                                      â•‘
â•‘  - Adapt to specific writing style                â•‘
â•‘  - Specialize in technical domain                 â•‘
â•‘  - Learn specific output format                   â•‘
â•‘  - Reduce hallucination in domain                 â•‘
â•‘  - Improve performance on narrow task             â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Fine-tuning = Specialized training after        â•‘
â•‘              general training to adapt to domain  â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 6, 10, (Advanced-6)            â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: TRAINING    â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£8ï¸âƒ£ | Pattern Matching vs Understanding | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ Do LLMs truly UNDERSTAND language?            â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ’¡ THE HONEST ANSWER:                              â•‘
â•‘  NOâ€”LLMs don't "understand" in the human sense.  â•‘
â•‘  They don't have:                                 â•‘
â•‘  â€¢ Consciousness                                  â•‘
â•‘  â€¢ Real comprehension                             â•‘
â•‘  â€¢ Intentional meaning-making                     â•‘
â•‘  â€¢ Genuine semantic understanding                 â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ WHAT THEY ACTUALLY ARE:                        â•‘
â•‘  Sophisticated PATTERN-MATCHING systems that      â•‘
â•‘  learned STATISTICAL RELATIONSHIPS in text.       â•‘
â•‘  They predict "what usually comes next"           â•‘
â•‘  based on patterns seen in training data.         â•‘
â•‘                                                     â•‘
â•‘  ğŸ­ THE ILLUSION:                                   â•‘
â•‘  LLMs BEHAVE as if they understand:               â•‘
â•‘  âœ“ Answer questions coherently                   â•‘
â•‘  âœ“ Follow instructions                           â•‘
â•‘  âœ“ Solve problems                                â•‘
â•‘  âœ“ Have conversations                            â•‘
â•‘  â†’ But this is sophisticated pattern-matching!    â•‘
â•‘                                                     â•‘
â•‘  âš™ï¸ THE MECHANISM:                                  â•‘
â•‘  No knowledge database                           â•‘
â•‘  No semantic representation                      â•‘
â•‘  Just: Input â†’ Mathematical operations            â•‘
â•‘        â†’ Probability distributions                â•‘
â•‘        â†’ Next token prediction                    â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  ANALOGY:                                        â•‘
â•‘  Imagine: Dictionary next to you at desk          â•‘
â•‘  Someone asks: "What comes after 'good'?"        â•‘
â•‘  You flip through patterns and say: "morning"    â•‘
â•‘  You don't UNDERSTAND "good morning"â€”just         â•‘
â•‘  recognized the pattern from your dictionary!     â•‘
â•‘  LLMs work similarly (at massive scale)          â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¡ KEY DISTINCTION:                                â•‘
â•‘  LLMs: Expert at pattern completion               â•‘
â•‘  Humans: Expert at meaning creation               â•‘
â•‘                                                     â•‘
â•‘  âš ï¸ THE DANGER:                                     â•‘
â•‘  LLMs seem to understand â†’ We trust them        â•‘
â•‘  But they're just matching patterns               â•‘
â•‘  â†’ Hallucinations result                         â•‘
â•‘  â†’ Factual errors occur                          â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  LLMs = Sophisticated pattern matchers that       â•‘
â•‘        appear intelligent but lack true           â•‘
â•‘        understanding or consciousness            â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 1, 11, 6                       â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: PHILOSOPHY  â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ¯ CARD 1ï¸âƒ£9ï¸âƒ£ | Transformer Architecture Introduction | â­â­â­ ESSENTIAL

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    FRONT (QUESTION)                â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  â“ When was the TRANSFORMER ARCHITECTURE         â•‘
â•‘     introduced and why is it important?           â•‘
â•‘                                                     â•‘
â•‘              ğŸ‘‡ FLIP TO REVEAL ANSWER ğŸ‘‡           â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                   BACK (ANSWER)                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                     â•‘
â•‘  ğŸ“… WHEN:                                           â•‘
â•‘  2017, in the paper "Attention Is All You Need"  â•‘
â•‘  by Vaswani and colleagues at Google Brain       â•‘
â•‘                                                     â•‘
â•‘  ğŸ¯ THE BREAKTHROUGH:                              â•‘
â•‘  This paper introduced a NEW neural network      â•‘
â•‘  architecture that became the foundation for     â•‘
â•‘  ALL modern Large Language Models:                â•‘
â•‘  â€¢ GPT series                                     â•‘
â•‘  â€¢ Claude                                         â•‘
â•‘  â€¢ LLaMA                                          â•‘
â•‘  â€¢ PaLM                                           â•‘
â•‘  â†’ All based on Transformer architecture          â•‘
â•‘                                                     â•‘
â•‘  ğŸ’¡ WHAT MADE IT REVOLUTIONARY:                    â•‘
â•‘  1. ATTENTION MECHANISMS                          â•‘
â•‘     Model can focus on relevant parts of text    â•‘
â•‘                                                     â•‘
â•‘  2. PARALLEL PROCESSING                           â•‘
â•‘     Can process all tokens simultaneously        â•‘
â•‘     (not sequential like older models)            â•‘
â•‘                                                     â•‘
â•‘  3. SCALABILITY                                    â•‘
â•‘     Could scale to billions of parameters        â•‘
â•‘     Older architectures couldn't scale           â•‘
â•‘                                                     â•‘
â•‘  ğŸ“Š IMPACT TIMELINE:                                â•‘
â•‘  2017: Transformer paper published               â•‘
â•‘  2018: BERT trained using Transformers           â•‘
â•‘  2018: GPT-1 released (first model)              â•‘
â•‘  2020: GPT-3 (175 billion parameters)            â•‘
â•‘  2022: ChatGPT launches (GPT-3.5)                â•‘
â•‘  2023: GPT-4, Claude, many others                â•‘
â•‘  â†’ Transformer era dominates AI                  â•‘
â•‘                                                     â•‘
â•‘  ğŸš€ WHY TRANSFORMERS ENABLED SCALING:             â•‘
â•‘  Old neural nets (RNNs): Process token-by-token  â•‘
â•‘                         (sequential, slow)       â•‘
â•‘  Transformers: Process all tokens in parallel    â•‘
â•‘               (much faster training!)            â•‘
â•‘  Result: Could train on trillions of tokens      â•‘
â•‘  â†’ Models could grow to billions of parameters   â•‘
â•‘                                                     â•‘
â•‘  ğŸ† SIGNIFICANCE:                                   â•‘
â•‘  This single architecture innovation enabled     â•‘
â•‘  the entire LLM revolution                       â•‘
â•‘  Every modern AI advancement traces back to this â•‘
â•‘                                                     â•‘
â•‘  ğŸ§  MEMORY AID:                                     â•‘
â•‘  Transformers (2017) = Foundation of modern LLMs â•‘
â•‘                      = Enabled scaling to billionsâ•‘
â•‘                      = Changed AI forever        â•‘
â•‘                                                     â•‘
â•‘  ğŸ”— RELATED CARDS: 3, 4, 1                        â•‘
â•‘  â­ DIFFICULTY: â˜…â˜†â˜† EASY | Category: HISTORY    â•‘
â•‘                                                     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“Œ Quick Reference Summary

| Card | Topic | Difficulty | Icon |
|------|-------|-----------|------|
| 1 | LLM Definition | â­â­â­ | ğŸ“– |
| 2 | Token | â­â­ | ğŸ”¹ |
| 3 | Transformer | â­â­â­ | âš™ï¸ |
| 4 | Attention | â­â­ | ğŸ‘ï¸ |
| 5 | Parameters | â­â­ | ğŸ“Š |
| 6 | Training Objective | â­â­â­ | ğŸ¯ |
| 7 | Model Size | â­â­ | ğŸ“ |
| 8 | Training Data | â­â­ | ğŸ’¾ |
| 9 | Inference | â­â­ | âš¡ |
| 10 | Hallucination | â­â­â­ | âš ï¸ |
| 11 | Scaling Laws | â­â­â­ | ğŸ“ˆ |
| 12 | Emergence | â­â­ | âœ¨ |
| 13 | Neural Networks | â­â­ | ğŸ§  |
| 14 | Context Length | â­â­ | ğŸ“ |
| 15 | Tokenization | â­â­ | ğŸ”„ |
| 16 | Knowledge Cutoff | â­â­â­ | ğŸ“… |
| 17 | Fine-tuning | â­â­ | ğŸ”§ |
| 18 | Pattern Matching | â­â­â­ | ğŸ­ |
| 19 | Transformers | â­â­â­ | ğŸš€ |

---

## ğŸ“ Study Instructions

1. **Read the FRONT side** - Memorize the question
2. **Think** - Try to recall the answer from memory
3. **Flip to BACK** - Check your answer
4. **Reflect** - Note the key points and memory aids
5. **Review** - Come back to difficult cards later

---

**âœ… Ready to continue to next cards?**

More cards coming in updated intermediate and advanced versions!

**Last Updated**: December 2024
**Format**: Visual Flashcard Layout
**Status**: âœ¨ Visually Attractive Design
